# Importing the dependencies.

# To perform numerical operations on matrices and arrays.
import numpy as np

# To handle dataframes and retrieve data from files.
import pandas as pd

# To interpret and visualise data in the form of |graphs and charts.
import matplotlib.pyplot as plt

#To access the contents of a folder.
import os

# Library containing Computer Vision tools to apply read, show, resize, etc operations to an image.
import cv2

# Confusion Matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from tensorflow.python.keras.layers import Dense, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

# Framework to train and run neural networks.
from tensorflow.keras import optimizers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import models
from tensorflow.keras import layers
#from keras.preprocessing.image import ImageDataGenerator
df=pd.read_csv("D:\Intern\DAR\state-farm-distracted-driver-detection\driver_imgs_list.csv") # Reading data.
df.head() # Printing head.
features=df["img"] # getting img column into features.
labels=df["classname"] # getting classname column into labels.
df["classname"].value_counts() # printing counts of each class.
df["classname"].value_counts().plot(kind='bar',color='g') # Visualising of the class-wise data
df["classname"].value_counts().plot(kind='pie') # Visualising of the class-wise data
path="D:/Intern/DAR/state-farm-distracted-driver-detection/imgs/train" # Path of the training images.
folder_names=os.listdir(path) # Getting the list of sub-folders in the training folder
folder_names
# Printing the total contents of each subfolder.
for i,folder in enumerate( folder_names):
    print(folder,"contains",len(os.listdir(path+"/"+folder)))
# Function to read images in color i.e in RGB mode.
def read_color():
    path="D:/Intern/DAR/state-farm-distracted-driver-detection/imgs/train" # Path
    image_data=[] # Array to store image data
    label_data=[] # Array to store label data
    for i in range(len (features)): # For each image
        img=cv2.imread(path+"/"+labels[i]+"/"+ features[i],cv2.IMREAD_COLOR)  # Reading it
        img = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2RGB) ,(64,64)) # Resizing and Switching channels from BGR to RGB.
        image_data.append(img) # Appending updated images in image_data array.
        label_data.append(labels[i]) # Appending updated labels in label_data array.
    return image_data,label_data # Returning variables.
# Function to split image_data and label_data for training and testing
def split(image_data,label_data):
    # Splitting the available data into training and testing data seperately in 80%-20%
    # train_images, train_labels will have values for TRAINING data.
    # images_validation_test, labels_validation_test will have values for VALIDATING data.
    train_images, images_validation_test, train_labels, labels_validation_test = train_test_split(
        image_data, label_data, test_size=0.2, random_state=50, stratify=labels)

    # Splitting the TESTING data into TESTING and VALIDATING data seperately in 50%-50%
    # i.e so that final ratio would be TRAINING (80%), VALIDATING (10%), TESTING (10%)
    # validation_images, validation_labels will have values for VALIDATING data.
    # test_images, test_labels will have values for TESTING data.
    validation_images, test_images, validation_labels, test_labels = train_test_split(
        images_validation_test, labels_validation_test, test_size=0.5,random_state=50,stratify=labels_validation_test)

    # Converting all of this data to arrays.
    train_images=np.asarray(train_images)
    validation_images=np.asarray(validation_images)
    test_images=np.asarray(test_images)
    train_labels=np.asarray(train_labels)
    validation_labels=np.asarray(validation_labels)
    test_labels=np.asarray(test_labels)

    # returning all variables after the split.
    return train_images,train_labels,validation_images,validation_labels,test_images,test_labels
# Function for pre-processing the feature variables before training.
def feature_preprocessing(train_images,validation_images,test_images):
    # Regularizing each pixel value so that the numerics fall in between 0 and 1.
    # So as to result in faster training period.

    train_images = train_images.astype('float32') / 255
    validation_images = validation_images.astype('float32') / 255
    test_images = test_images.astype('float32') / 255

    # returning all pre-processed variables.
    return train_images,validation_images,test_images
# Function for pre-processing the label variables before training.
def label_preprocessing(train_labels,validation_labels,test_labels):
    # Loading label encoder to convert labels into numeric form to make them machine-readable.
    label_encoder = LabelEncoder()

    # Doing fit transform for standardization of the respective values --1
    # And then changing the respective class variables to integers using to_categorical. --2
    vec = label_encoder.fit_transform(train_labels)      # 1
    train_labels = to_categorical(vec)                   # 2
    vec = label_encoder.fit_transform(validation_labels) # 1
    validation_labels = to_categorical(vec)              # 2
    vec = label_encoder.fit_transform(test_labels)       # 1
    test_labels = to_categorical(vec)                    # 2

    # returning all pre-processed variables.
    return train_labels,validation_labels,test_labels
# Getting data into respective by calling the corresponding functions.
image_data, label_data=read_color()
train_images,train_labels,validation_images,validation_labels,test_images,test_labels=split(image_data,label_data)
train_labels,validation_labels,test_labels=label_preprocessing(train_labels,validation_labels,test_labels)
train_images,validation_images,test_images=feature_preprocessing(train_images,validation_images,test_images)
# Applying the model.
# Performing  2D convolution and Max Pooling with relu activation function

network_cnn = models.Sequential()
network_cnn.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(64,64,3)))
network_cnn.add(layers.MaxPooling2D((2, 2)))
network_cnn.add(layers.Conv2D(32, (3, 3), activation='relu'))
network_cnn.add(layers.MaxPooling2D((2, 2)))
network_cnn.add(layers.Conv2D(32, (3, 3), activation='relu'))
network_cnn.add(layers.Flatten())
network_cnn.add(layers.Dense(128, activation='relu'))
network_cnn.add(layers.Dense(10, activation='softmax'))
# Printing the network summary.
network_cnn.summary()
# Performing the model training.
# network_cnn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
network_cnn.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])
model_cnn=network_cnn.fit(train_images, train_labels,
                          validation_data=(validation_images,validation_labels),epochs=20, batch_size=128)
# Getting the accuracy of our model.
test_loss, test_acc = network_cnn.evaluate(test_images, test_labels)
print("Accuracy of the proposed model:",test_acc*100,"%")
# Printing the predicted values and actual values for test images.
prediction = network_cnn.predict(test_images)
pred_vals = []
for x in prediction:
    pred_vals.append(np.argmax(x))
test_vals = []
for x in test_labels:
    test_vals.append(np.where(x==(x.max()))[0][0])
print(len(pred_vals),len(test_labels),pred_vals,test_vals,sep="\n\n\n")
# Plotting the confusion matrix.
cf_matrix = confusion_matrix(test_vals,pred_vals)
sns.heatmap(cf_matrix, fmt='d',annot=True,cmap='Blues')
# Plotting the curves for accuracy and loss.

acc = model_cnn.history['accuracy']
val_acc = model_cnn.history['val_accuracy']
loss = model_cnn.history['loss']
val_loss = model_cnn.history['val_loss']

iters = range(len(acc))
plt.plot(iters, acc, '*', label='Training Accuracy')
plt.plot(iters, val_acc, '-', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.figure()

plt.plot(iters, loss, '*', label='Training Loss')
plt.plot(iters, val_loss, '-', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.figure()

plt.show()


# Applying DenseNet121 Model.
Dense_net = models.Sequential()
pretrained_model = tf.keras.applications.DenseNet121(include_top=False,
                                                  input_shape=(64,64,3),
                                                  pooling='avg',
                                                  classes=10,
                                                  weights='imagenet')
for layer in pretrained_model.layers:
    layer.trainable=False   # Making those layers untrainable so that weights doesnt change anymore.

Dense_net.add(pretrained_model)
Dense_net.add(Flatten())
Dense_net.add(Dense(128,activation='relu'))
Dense_net.add(Dense(10,activation='softmax'))
Dense_net.summary()
Dense_net.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])
epochs = 25
history = Dense_net.fit(train_images, train_labels,
                          validation_data=(validation_images,validation_labels),epochs=epochs, batch_size=128)
# Getting the accuracy of DenseNet model.
test_loss, test_acc = Dense_net.evaluate(test_images, test_labels)
print("Accuracy of the densenet model:",test_acc*100,"%")
# Printing the predicted values and actual values for test images
prediction = Dense_net.predict(test_images)
pred_vals = []
for x in prediction:
    pred_vals.append(np.argmax(x))
test_vals = []
for x in test_labels:
    test_vals.append(np.where(x==(x.max()))[0][0])
print(len(pred_vals),len(test_labels),pred_vals,test_vals,sep="\n\n\n")
# Plotting the confusion matrix
cf_matrix = confusion_matrix(test_vals,pred_vals)
sns.heatmap(cf_matrix, fmt='d',annot=True,cmap='Blues')
# Plotting the curves for accuracy and loss

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

iters = range(len(acc))
plt.plot(iters, acc, '*', label='Training Accuracy')
plt.plot(iters, val_acc, '-', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.figure()

plt.plot(iters, loss, '*', label='Training Loss')
plt.plot(iters, val_loss, '-', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.figure()

plt.show()

# Applying VGG16 Model
vgg_net = models.Sequential()
pretrained_model = tf.keras.applications.VGG16(include_top=False,
                                                  input_shape=(64,64,3),
                                                  pooling='avg',
                                                  classes=10,
                                                  weights='imagenet')
for layer in pretrained_model.layers:
    layer.trainable=False   # Making those layers untrainable so that weights doesnt change anymore.

vgg_net.add(pretrained_model)
vgg_net.add(Flatten())
vgg_net.add(Dense(128,activation='relu'))
vgg_net.add(Dense(10,activation='softmax'))
vgg_net.compile(optimizer=Adam(lr=0.01),loss='categorical_crossentropy',metrics=['accuracy'])
epochs = 10
history = vgg_net.fit(train_images, train_labels,
                          validation_data=(validation_images,validation_labels),epochs=epochs, batch_size=128)


**ENSEMBLE MODEL**
# Resnet Model Predictions:
prediction_densenet = Dense_net.predict(test_images)

# VGG16 Model Predictions:
prediction_mycnn = network_cnn.predict(test_images)

# Ensemble Model Predictions:
prediction_ensemble = (prediction_densenet+prediction_mycnn)/2


pred_vals = []
for x in prediction_ensemble:
    pred_vals.append(np.argmax(x))

test_vals = []
for x in test_labels:
    test_vals.append(np.where(x==(x.max()))[0][0])

print(len(pred_vals),len(test_labels),pred_vals,test_vals,pred_vals[0],test_vals[0],sep="\n\n\n")
# Accuracy
correct_classifications=0
for i in range(len(test_images)):
    if pred_vals[i]==test_vals[i]:
        correct_classifications=correct_classifications+1
print("\n\nAccuracy of the ensemble model:",(correct_classifications/len(test_images))*100)
# Plotting the confusion matrix for VGG16 model for Exp-1.
cf_matrix = confusion_matrix(test_vals,pred_vals)
sns.heatmap(cf_matrix, fmt='d',annot=True,cmap='Reds')

from keras.preprocessing import image
import tensorflow
import numpy as np
# Function for image pre-processing.
def image_preprocessing(img_path):
    img = tensorflow.keras.preprocessing.image.load_img(img_path, target_size=(64, 64)) # Resizing the image
    img_preproc = tensorflow.keras.preprocessing.image.img_to_array(img) # Changing it to array
    img_preproc = np.expand_dims(img_preproc, axis=0) # Expanding dimensions
    img_preproc /= 255. # Regularizing

    # returning all pre-processed variables.
    return img_preproc
img_path="D:\Intern\DAR\state-farm-distracted-driver-detection\imgs/train/c3/img_1317.jpg"
img_preproc = image_preprocessing(img_path)
from keras import models
# Extracts the outputs of the top 8 layers:
layer_outputs = [layer.output for layer in network_cnn.layers[:5]]
# Creates a model that will return these outputs, given the model input:
activation_model = models.Model(inputs=network_cnn.input, outputs=layer_outputs)
activations = activation_model.predict(img_preproc)
plt.imshow(img_preproc[0])
plt.show()
len(activations)
first_layer_activation = activations[0]
print(first_layer_activation.shape)
first_layer_activation = activations[0]
print(first_layer_activation.shape)
network_cnn.layers[:4]
import keras

# These are the names of the layers, so can have them as part of our plot
layer_names = []
for layer in network_cnn.layers[:5]:
    layer_names.append(layer.name)

images_per_row = 16

# Now let's display our feature maps
for layer_name, layer_activation in zip(layer_names, activations):
    # This is the number of features in the feature map
    print(layer_activation.shape)
    n_features = layer_activation.shape[-1]
    # The feature map has shape (1, size, size, n_features)
    size = layer_activation.shape[1]
    # We will tile the activation channels in this matrix
    n_cols = n_features // images_per_row
    display_grid = np.zeros((size * n_cols, images_per_row * size))

    # We'll tile each filter into this big horizontal grid
    for col in range(n_cols):
        for row in range(images_per_row):
            channel_image = layer_activation[0,
                                             :, :,
                                             col * images_per_row + row]
            # Post-process the feature to make it visually palatable
            channel_image -= channel_image.mean()
            channel_image /= channel_image.std()
            channel_image *= 64
            channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype('uint8')
            display_grid[col * size : (col + 1) * size,
                         row * size : (row + 1) * size] = channel_image

    # Display the grid
    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, aspect='auto', cmap='viridis')

plt.show()
from keras import backend as K
K.clear_session()
# Function to display outputs of sample test images.
def testing_output(img_preproc,class_names):

    # Printing Parameters
    plt.imshow(img_preproc[0]) # Printing image
    plt.show()

    #print(img_preproc.shape) # Printing its shape now.

    prediction = network_cnn.predict(img_preproc) # Predicting the output
    #print(prediction) # Printing the prediction values.

    max_val = np.argmax(prediction)
    #print(max_val) # Printing the max prediction value.

    class_out = class_names[max_val]
    print(class_out) # Printing the index of the max prediction value.
# Enter Path here.
img_path="D:\Intern\DAR\state-farm-distracted-driver-detection\imgs/train/c1/img_1950.jpg"

# Enter Class names here.
class_names = ["Normal Driving","Texting Right","Calling Right","Texting Left","Calling Left"
                   ,"Checking Radio","Drinking","Looking Behind","Makeup","Talking with co-passenger"]

img_preproc = image_preprocessing(img_path)
testing_output(img_preproc,class_names)
